# VQ-VAE Training Config with Adaptive Compression Ratios
#
# This config demonstrates the use of adaptive per-category compression ratios.
# The system automatically analyzes feature characteristics (variance, dimensionality,
# information content, correlation) and computes optimal compression ratios for each
# category to improve reconstruction quality.
#
# Key innovation: Instead of uniform compression ratios like [0.5, 1.0, 1.5],
# each category gets custom ratios based on its complexity:
# - High-variance categories (e.g., temporal sequences) → less compression
# - High-redundancy categories (e.g., correlated features) → more compression
# - High-dimensional categories → adaptive bottleneck

# Dataset
dataset_path: "datasets/100k_full_features.h5"
max_samples: 10000  # Use 10K for fast validation

# Feature Families
families:
  initial:
    encoder: initial_hybrid
    encoder_params:
      cnn_embedding_dim: 28
      manual_dim: 14
      in_channels: 1
      encode_manual: false

  summary:
    encoder: MLPEncoder
    encoder_params:
      hidden_dims: [512, 256]
      output_dim: 128
      dropout: 0.1
      activation: "relu"
      batch_norm: true

  temporal:
    encoder: TemporalCNNEncoder
    encoder_params:
      architecture: "resnet1d_3"
      embedding_dim: 128

# VQ-VAE Architecture
model:
  group_embedding_dim: 256
  group_hidden_dim: 128

  # ADAPTIVE COMPRESSION RATIOS
  # Setting this to "auto" triggers adaptive computation per category
  # Based on diagnostic results showing cluster_2 has 46,578x higher error than cluster_4,
  # adaptive ratios will allocate more capacity to high-error categories
  compression_ratios: "auto"

  # Strategy for adaptive computation (default: balanced)
  # Options:
  #   - "balanced": Weighted combination of variance, dimensionality, information
  #   - "variance": Prioritizes high-variance features (good for temporal data)
  #   - "dimensionality": Based on feature count (high-dim → compress more)
  #   - "information": Based on redundancy (high correlation/PCA → compress more)
  auto_compression_strategy: "balanced"

  # VQ-VAE hyperparameters
  commitment_cost: 0.45
  use_ema: true
  decay: 0.99
  dropout: 0.1

  # Auxiliary loss weights
  orthogonality_weight: 0.1
  informativeness_weight: 0.1

# Feature Cleaning (applied before adaptive ratio computation)
feature_cleaning:
  enabled: true
  variance_threshold: 1.0e-10  # Remove zero-variance features
  deduplicate_threshold: 0.99  # Remove highly correlated features
  use_intelligent_dedup: true
  outlier_method: "percentile"
  percentile_range: [0.5, 99.5]
  mad_threshold: 3.0

# Training
training:
  batch_size: 512
  learning_rate: 0.0007
  num_epochs: 100
  optimizer: "adam"
  scheduler: null
  warmup_epochs: 0

  # Category assignment - AUTOMATIC via clustering
  category_assignment: "auto"
  num_categories_auto: null  # Auto-determine via silhouette score
  orthogonality_target: 0.15
  min_features_per_category: 3
  max_clusters: 25

  # Loss weights
  reconstruction_weight: 1.0
  vq_weight: 1.0
  orthogonality_weight: 0.1
  informativeness_weight: 0.1
  topo_weight: 0.02
  topo_samples: 64

  # Checkpointing
  save_every: null  # Only save best
  checkpoint_dir: "checkpoints/vqvae_adaptive_compression"

  # Callbacks
  early_stopping_patience: 50
  early_stopping_min_delta: 0.01
  dead_code_reset_interval: null  # Disabled for adaptive compression test
  dead_code_threshold: 10.0
  dead_code_max_reset_fraction: 0.25

  # Validation
  val_every_n_epochs: 5

  # Performance
  use_torch_compile: false

# Logging
logging:
  wandb: false
  log_interval: 50
  eval_interval: 200
  verbose: true

# Random seed
random_seed: 42

# Expected Behavior:
# ===================
# Based on diagnostic results, the system will compute adaptive ratios like:
#
# cluster_2 (33D, MSE=3611.91, highest error):
#   Adaptive ratios: [0.3, 0.8, 1.8] → MORE capacity (less compression)
#   Rationale: High reconstruction error indicates underfitting
#
# cluster_1 (16D, MSE=0.12, lowest error):
#   Adaptive ratios: [0.6, 1.3, 2.0] → LESS capacity (more compression)
#   Rationale: Already reconstructing well, can free up capacity
#
# This rebalancing should:
# 1. Reduce cluster_2 error by 20-30%
# 2. Maintain cluster_1 quality
# 3. Improve overall codebook utilization (currently 20.4% → target 40%+)
