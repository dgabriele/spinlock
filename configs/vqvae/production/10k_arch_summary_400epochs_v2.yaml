# Production Training V2: ARCHITECTURE + SUMMARY Joint Training (Full 10K Dataset, 400 Epochs)
# IMPROVEMENTS OVER V1:
# - Increased batch_size: 256 → 512 (better GPU utilization)
# - Smart dead code resets: Condition-based instead of fixed-interval
# - Expected benefits: No disruptive resets after convergence, faster training

# Dataset
dataset_path: "datasets/baseline_10k.h5"
# No max_samples limit - use full dataset

# Feature Families
families:
  architecture:
    encoder: IdentityEncoder
    encoder_params:
      input_dim: 31  # ARCHITECTURE parameter features

  summary:
    encoder: MLPEncoder
    encoder_params:
      hidden_dims: [512, 256]  # Increased from [256, 128] for high-GPU mode
      output_dim: 128  # Increased from 64
      dropout: 0.1
      activation: "relu"
      batch_norm: true

# VQ-VAE Architecture
model:
  group_embedding_dim: 256  # Kept at 256
  group_hidden_dim: 512

  # Hierarchical levels (auto-discovered categories with auto-scaled dimensions)
  # Leave as null to auto-create per-category levels based on compression_ratios
  levels: null

  # VQ-VAE hyperparameters
  commitment_cost: 0.5  # Increased from 0.25 (prevent codebook collapse)
  use_ema: true
  decay: 0.99
  dropout: 0.1

  # Auxiliary loss weights
  orthogonality_weight: 0.1
  informativeness_weight: 0.1

# Training
training:
  batch_size: 512  # V2: Increased from 256 (2× larger for better GPU utilization)
  learning_rate: 0.001
  num_epochs: 400  # Extended for comprehensive training on full dataset
  optimizer: "adam"
  scheduler: null
  warmup_epochs: 0

  # Compression ratios for latent_dim autoscaling (per-category)
  # Ratios [L0:L1:L2] relative to each category's feature_count
  # E.g., "0.5:1:1.5":
  #   - Category with 7 features:  L0=4D,  L1=8D,  L2=12D
  #   - Category with 71 features: L0=36D, L1=72D, L2=108D
  # This makes latent space scale with category complexity
  compression_ratios: "0.5:1:1.5"

  # Category discovery (auto mode)
  category_assignment: "auto"
  num_categories_auto: null  # Auto-determine via silhouette score
  orthogonality_target: 0.15
  min_features_per_category: 3
  max_clusters: 18  # Balanced: not too many (25) or too few (12)

  # Loss weights
  reconstruction_weight: 1.0
  vq_weight: 1.0
  orthogonality_weight: 0.1
  informativeness_weight: 0.1
  topo_weight: 0.45  # Increased from 0.02 (match unisim for better topology preservation)
  topo_samples: 1024  # Increased from 512 (match unisim)

  # Checkpointing
  save_every: 50  # Save checkpoint every 50 epochs
  checkpoint_dir: "checkpoints/production/10k_arch_summary_400epochs_v2"

  # Callbacks
  early_stopping_patience: 50  # Increased patience for long training
  early_stopping_min_delta: 0.01

  # V2: Smart dead code reset (condition-based, adaptive)
  use_smart_reset: true  # Enable intelligent SmartDeadCodeReset
  dead_code_threshold: 10.0  # Base percentile threshold
  # Note: dead_code_reset_interval ignored when use_smart_reset=true
  # Smart reset will only trigger when:
  # - Utilization < 25% AND declining
  # - Training not actively improving
  # - Adaptive intervals: 50→100→200 epochs
  # - Stops after convergence (early_stopping_counter > 30)

  # Validation
  val_every_n_epochs: 10  # Reduce validation overhead for long training

  # Performance
  use_torch_compile: true  # Enable torch.compile for additional speedup

# Logging
logging:
  wandb: false
  log_interval: 100
  eval_interval: 500
  verbose: true

# Random seed
random_seed: 42

# Expected Performance (V3 - Per-category latent dimension scaling):
# - GPU utilization: 70-90%
# - Model parameters: ~5-20M (depends on auto-discovered categories)
# - Training time: ~15-20 minutes
#
# Key Design Choices:
# 1. compression_ratios="0.5:1:1.5" → per-category scaling
#    - Each category's latent_dim = feature_count × compression_ratio
#    - Smaller categories get smaller latent spaces (e.g., 7 feat → 4D/8D/12D)
#    - Larger categories get larger latent spaces (e.g., 71 feat → 36D/72D/108D)
#    - Latent space scales with category complexity (not uniform!)
# 2. commitment_cost=0.5 (vs 0.25 default) → prevent codebook collapse
# 3. topo_weight=0.45 (vs 0.02 default) → better topology preservation
# 4. SmartDeadCodeReset improvements:
#    - Detects catastrophic collapse (>5% decline in 10 epochs)
#    - Emergency reset when util <15%
#    - Resets on stable low utilization (not just declining)
#
# Target Metrics:
#   - Quality: >0.90 (composite: reconstruction + utilization + balance)
#   - Codebook utilization: >40% (healthy codebook usage)
#   - Category orthogonality: <0.25
#   - Topographic correlation: >0.60
