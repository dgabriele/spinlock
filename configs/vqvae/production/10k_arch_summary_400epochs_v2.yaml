# Production Training V2: ARCHITECTURE + SUMMARY Joint Training (Full 10K Dataset, 400 Epochs)
# IMPROVEMENTS OVER V1:
# - Increased batch_size: 256 → 512 (better GPU utilization)
# - Smart dead code resets: Condition-based instead of fixed-interval
# - Expected benefits: No disruptive resets after convergence, faster training

# Dataset
dataset_path: "datasets/baseline_10k.h5"
# No max_samples limit - use full dataset

# Feature Families
families:
  architecture:
    encoder: IdentityEncoder
    encoder_params:
      input_dim: 31  # ARCHITECTURE parameter features

  summary:
    encoder: MLPEncoder
    encoder_params:
      hidden_dims: [512, 256]  # Increased from [256, 128] for high-GPU mode
      output_dim: 128  # Increased from 64
      dropout: 0.1
      activation: "relu"
      batch_norm: true

# VQ-VAE Architecture
model:
  group_embedding_dim: 256  # Kept at 256
  group_hidden_dim: 512

  # Hierarchical levels (empty for autoscaling)
  levels: []

  # VQ-VAE hyperparameters
  commitment_cost: 0.5  # Increased from 0.25 (prevent codebook collapse)
  use_ema: true
  decay: 0.99
  dropout: 0.1

  # Auxiliary loss weights
  orthogonality_weight: 0.1
  informativeness_weight: 0.1

# Training
training:
  batch_size: 512  # V2: Increased from 256 (2× larger for better GPU utilization)
  learning_rate: 0.001
  num_epochs: 400  # Extended for comprehensive training on full dataset
  optimizer: "adam"
  scheduler: null
  warmup_epochs: 0

  # Compression ratios for latent_dim autoscaling (from unisim)
  # Ratios [L0:L1:L2] relative to group_embedding_dim
  # E.g., "0.5:1:1.5" with embedding_dim=256 → [128, 256, 384] latent_dims
  compression_ratios: "0.5:1:1.5"

  # Category discovery (auto mode)
  category_assignment: "auto"
  num_categories_auto: null  # Auto-determine via silhouette score
  orthogonality_target: 0.15
  min_features_per_category: 3
  max_clusters: 18  # Balanced: not too many (25) or too few (12)

  # Loss weights
  reconstruction_weight: 1.0
  vq_weight: 1.0
  orthogonality_weight: 0.1
  informativeness_weight: 0.1
  topo_weight: 0.45  # Increased from 0.02 (match unisim for better topology preservation)
  topo_samples: 1024  # Increased from 512 (match unisim)

  # Checkpointing
  save_every: 50  # Save checkpoint every 50 epochs
  checkpoint_dir: "checkpoints/production/10k_arch_summary_400epochs_v2"

  # Callbacks
  early_stopping_patience: 50  # Increased patience for long training
  early_stopping_min_delta: 0.01

  # V2: Smart dead code reset (condition-based, adaptive)
  use_smart_reset: true  # Enable intelligent SmartDeadCodeReset
  dead_code_threshold: 10.0  # Base percentile threshold
  # Note: dead_code_reset_interval ignored when use_smart_reset=true
  # Smart reset will only trigger when:
  # - Utilization < 25% AND declining
  # - Training not actively improving
  # - Adaptive intervals: 50→100→200 epochs
  # - Stops after convergence (early_stopping_counter > 30)

  # Validation
  val_every_n_epochs: 10  # Reduce validation overhead for long training

  # Performance
  use_torch_compile: true  # Enable torch.compile for additional speedup

# Logging
logging:
  wandb: false
  log_interval: 100
  eval_interval: 500
  verbose: true

# Random seed
random_seed: 42

# Expected Performance (V3 - Fixed with unisim-style config):
# - GPU utilization: 70-90%
# - Model parameters: ~20M
# - Training time: ~15-20 minutes
#
# Key Fixes vs V2:
# 1. compression_ratios="0.5:1:1.5" → proper latent_dim scaling
#    - V2: [652, 328, 164] latent_dims (way too large!)
#    - V3: [128, 256, 384] latent_dims (appropriate)
# 2. commitment_cost=0.5 (vs 0.25) → prevent codebook collapse
# 3. topo_weight=0.45 (vs 0.02) → better topology preservation
# 4. SmartDeadCodeReset improvements:
#    - Detects catastrophic collapse (>5% decline in 10 epochs)
#    - Emergency reset when util <15%
#    - Resets on stable low utilization (not just declining)
#
# Target Metrics:
#   - Quality: >0.90 (vs 0.30 in V2 - major fix needed!)
#   - Codebook utilization: >30% (vs 12.98% in V2)
#   - Category orthogonality: <0.25
#   - Topographic correlation: >0.60
