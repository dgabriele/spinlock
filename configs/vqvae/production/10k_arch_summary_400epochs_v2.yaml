# Production Training V2: ARCHITECTURE + SUMMARY Joint Training (Full 10K Dataset, 400 Epochs)
# IMPROVEMENTS OVER V1:
# - Increased batch_size: 256 → 512 (better GPU utilization)
# - Smart dead code resets: Condition-based instead of fixed-interval
# - Expected benefits: No disruptive resets after convergence, faster training

# Dataset
dataset_path: "datasets/baseline_10k.h5"
# No max_samples limit - use full dataset

# Feature Families
families:
  architecture:
    encoder: MLPEncoder
    encoder_params:
      hidden_dims: [256, 128]  # Increased from [128, 64] for high-GPU mode
      output_dim: 128  # Increased from 64
      dropout: 0.1
      activation: "relu"
      batch_norm: true

  summary:
    encoder: MLPEncoder
    encoder_params:
      hidden_dims: [512, 256]  # Increased from [256, 128] for high-GPU mode
      output_dim: 128  # Increased from 64
      dropout: 0.1
      activation: "relu"
      batch_norm: true

# VQ-VAE Architecture
model:
  group_embedding_dim: 256  # Increased from 64 (4× larger for high-GPU mode)
  group_hidden_dim: 512  # Increased from 128 (4× larger)

  # Hierarchical levels (empty for autoscaling)
  levels: []

  # VQ-VAE hyperparameters
  commitment_cost: 0.25
  use_ema: true
  decay: 0.99
  dropout: 0.1

  # Auxiliary loss weights
  orthogonality_weight: 0.1
  informativeness_weight: 0.1

# Training
training:
  batch_size: 512  # V2: Increased from 256 (2× larger for better GPU utilization)
  learning_rate: 0.001
  num_epochs: 400  # Extended for comprehensive training on full dataset
  optimizer: "adam"
  scheduler: null
  warmup_epochs: 0

  # Category discovery (auto mode)
  category_assignment: "auto"
  num_categories_auto: null  # Auto-determine via silhouette score
  orthogonality_target: 0.15
  min_features_per_category: 3
  max_clusters: 25

  # Loss weights
  reconstruction_weight: 1.0
  vq_weight: 1.0
  orthogonality_weight: 0.1
  informativeness_weight: 0.1
  topo_weight: 0.02
  topo_samples: 512  # Increased from 64 (8× larger for better topographic loss)

  # Checkpointing
  save_every: 50  # Save checkpoint every 50 epochs
  checkpoint_dir: "checkpoints/production/10k_arch_summary_400epochs_v2"

  # Callbacks
  early_stopping_patience: 50  # Increased patience for long training
  early_stopping_min_delta: 0.01

  # V2: Smart dead code reset (condition-based, adaptive)
  use_smart_reset: true  # Enable intelligent SmartDeadCodeReset
  dead_code_threshold: 10.0  # Base percentile threshold
  # Note: dead_code_reset_interval ignored when use_smart_reset=true
  # Smart reset will only trigger when:
  # - Utilization < 25% AND declining
  # - Training not actively improving
  # - Adaptive intervals: 50→100→200 epochs
  # - Stops after convergence (early_stopping_counter > 30)

  # Validation
  val_every_n_epochs: 10  # Reduce validation overhead for long training

  # Performance
  use_torch_compile: true  # Enable torch.compile for additional speedup

# Logging
logging:
  wandb: false
  log_interval: 100
  eval_interval: 500
  verbose: true

# Random seed
random_seed: 42

# Expected Performance (V2):
# - GPU utilization: 70-90% (vs 60-80% in V1 due to larger batch)
# - Model parameters: ~20M (same as V1)
# - Training time: ~15-20 minutes (faster due to larger batch, no disruptive resets)
# - Target metrics:
#   - Reconstruction quality: >0.90
#   - Codebook utilization: >25%
#   - Category orthogonality: <0.25
#   - Topographic correlation: >0.60
# - Key improvement: No harmful resets after convergence (~epoch 100)
