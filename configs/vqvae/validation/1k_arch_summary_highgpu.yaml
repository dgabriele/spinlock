# Stage 2A: ARCHITECTURE + SUMMARY Joint Training (High GPU Utilization)
# Optimized for maximum GPU usage with larger batch size and model capacity

# Dataset
dataset_path: "datasets/baseline_10k.h5"
max_samples: 1000

# Feature Families
families:
  architecture:
    encoder: MLPEncoder
    encoder_params:
      hidden_dims: [256, 128]  # Increased from [128, 64]
      output_dim: 128  # Increased from 64
      dropout: 0.1
      activation: "relu"
      batch_norm: true

  summary:
    encoder: MLPEncoder
    encoder_params:
      hidden_dims: [512, 256]  # Increased from [256, 128]
      output_dim: 128  # Increased from 64
      dropout: 0.1
      activation: "relu"
      batch_norm: true

# VQ-VAE Architecture
model:
  group_embedding_dim: 256  # Increased from 64 (4× larger)
  group_hidden_dim: 512  # Increased from 128 (4× larger)

  # Hierarchical levels (empty for autoscaling)
  levels: []

  # VQ-VAE hyperparameters
  commitment_cost: 0.25
  use_ema: true
  decay: 0.99
  dropout: 0.1

  # Auxiliary loss weights
  orthogonality_weight: 0.1
  informativeness_weight: 0.1

# Training
training:
  batch_size: 256  # Increased from 32 (8× larger for better GPU utilization)
  learning_rate: 0.001
  num_epochs: 150
  optimizer: "adam"
  scheduler: null
  warmup_epochs: 0

  # Loss weights
  reconstruction_weight: 1.0
  vq_weight: 1.0
  orthogonality_weight: 0.1
  informativeness_weight: 0.1
  topo_weight: 0.02
  topo_samples: 512  # Increased from 64 (8× larger for better topographic loss)

  # Checkpointing
  save_every: 10
  checkpoint_dir: "checkpoints/validation/1k_arch_summary_highgpu"

  # Callbacks
  early_stopping_patience: 20
  early_stopping_min_delta: 0.01
  dead_code_reset_interval: 50
  dead_code_threshold: 10.0
  dead_code_max_reset_fraction: 0.25

  # Validation
  val_every_n_epochs: 5

  # Performance
  use_torch_compile: true  # Enable torch.compile for additional speedup

# Logging
logging:
  wandb: false
  log_interval: 100
  eval_interval: 500
  verbose: true

# Random seed
random_seed: 42

# Expected GPU utilization: 60-80% (vs 20-30% with standard config)
# Model parameters: ~20M (vs ~4.8M with standard config)
# Training speed: Faster per epoch due to batching, fewer total epochs needed
