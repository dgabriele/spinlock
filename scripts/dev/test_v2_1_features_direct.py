#!/usr/bin/env python
"""
Direct empirical validation of SDF v2.1 features without full dataset generation.

Generates synthetic test data and validates Phase 1 + Phase 2 fixes.
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))

import torch
import numpy as np
from typing import Dict


def generate_test_trajectories(
    num_operators: int = 20,
    num_realizations: int = 3,
    num_timesteps: int = 200,
    num_channels: int = 3,
    grid_size: int = 64
) -> torch.Tensor:
    """Generate diverse synthetic test trajectories."""
    print(f"\nGenerating {num_operators} test operators...")
    print(f"  Shape: [N={num_operators}, M={num_realizations}, T={num_timesteps}, C={num_channels}, H=W={grid_size}]")

    trajectories = []

    for i in range(num_operators):
        # Create diverse dynamics types
        if i % 5 == 0:
            # Constant field (tests zero-variance handling)
            traj = torch.ones(num_realizations, num_timesteps, num_channels, grid_size, grid_size) * (0.5 + i * 0.01)

        elif i % 5 == 1:
            # Random walk (tests temporal features)
            traj = torch.cumsum(torch.randn(num_realizations, num_timesteps, num_channels, grid_size, grid_size) * 0.1, dim=1)

        elif i % 5 == 2:
            # Oscillatory (tests spectral features)
            t = torch.linspace(0, 10 * np.pi, num_timesteps).reshape(1, num_timesteps, 1, 1, 1)
            spatial = torch.randn(num_realizations, 1, num_channels, grid_size, grid_size)
            traj = torch.sin(t) * spatial

        elif i % 5 == 3:
            # Smooth gradient (tests spatial features)
            x = torch.linspace(0, 1, grid_size).reshape(1, 1, 1, 1, grid_size)
            y = torch.linspace(0, 1, grid_size).reshape(1, 1, 1, grid_size, 1)
            t = torch.linspace(0, 1, num_timesteps).reshape(1, num_timesteps, 1, 1, 1)
            traj = (x + y + t).expand(num_realizations, num_timesteps, num_channels, grid_size, grid_size)

        else:
            # Heavy-tailed noise (tests kurtosis/skewness handling)
            # Mix of Gaussian + occasional large spikes
            base = torch.randn(num_realizations, num_timesteps, num_channels, grid_size, grid_size) * 0.5
            spikes = (torch.rand(num_realizations, num_timesteps, num_channels, grid_size, grid_size) > 0.95).float() * 10
            traj = base + spikes

        trajectories.append(traj)

    trajectories = torch.stack(trajectories)  # [N, M, T, C, H, W]

    print(f"‚úì Generated test data: {trajectories.shape}")
    return trajectories


def test_feature_extraction():
    """Test feature extraction with all extractors enabled."""
    print("="*70)
    print("SDF v2.1 Direct Feature Extraction Test")
    print("="*70)

    from spinlock.features.sdf import SDFExtractor, SDFConfig

    # Create config with all extractors enabled
    config = SDFConfig()

    # Enable Phase 2 extractors explicitly
    config.distributional.enabled = True
    config.structural.enabled = True
    config.physics.enabled = True
    config.morphological.enabled = True
    config.multiscale.enabled = True

    # Disable slow extractors for faster testing
    config.operator_sensitivity.enabled = False
    config.causality.enabled = False

    print("\n‚úì Created SDF config with all Phase 2 extractors enabled")

    # Initialize extractor
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    extractor = SDFExtractor(device=device, config=config)

    print(f"‚úì Initialized SDF extractor on {device}")

    # Generate test data (reduced for faster testing)
    num_operators = 10  # Reduced from 20 for speed
    trajectories = generate_test_trajectories(
        num_operators=num_operators,
        num_realizations=3,
        num_timesteps=200,
        num_channels=3,
        grid_size=64
    )

    # Extract features with batch size 1 (process one operator at a time)
    print("\n" + "="*70)
    print("Extracting features (batch size 1 for memory efficiency)...")
    print("="*70)

    try:
        all_aggregated_mean = []
        all_aggregated_std = []
        all_aggregated_cv = []

        for i in range(num_operators):
            print(f"  Processing operator {i+1}/{num_operators}...", end='\r')

            # Extract single operator [1, M, T, C, H, W]
            single_traj = trajectories[i:i+1].to(device)

            # Extract features for this operator
            result = extractor.extract_all(single_traj)

            # Collect aggregated features
            all_aggregated_mean.append(result['aggregated_mean'].cpu())
            all_aggregated_std.append(result['aggregated_std'].cpu())
            all_aggregated_cv.append(result['aggregated_cv'].cpu())

            # Free GPU memory
            del single_traj, result
            torch.cuda.empty_cache()

        print(f"  Processing operator {num_operators}/{num_operators}... Done!")

        # Concatenate all results
        aggregated_mean = torch.cat(all_aggregated_mean, dim=0)  # [N, num_features]
        aggregated_std = torch.cat(all_aggregated_std, dim=0)
        aggregated_cv = torch.cat(all_aggregated_cv, dim=0)

        print(f"\n‚úì Feature extraction successful!")
        print(f"  aggregated_mean: {aggregated_mean.shape}")
        print(f"  aggregated_std: {aggregated_std.shape}")
        print(f"  aggregated_cv: {aggregated_cv.shape}")

        features = torch.cat([aggregated_mean, aggregated_std, aggregated_cv], dim=1)

        result = {
            'aggregated_mean': aggregated_mean,
            'aggregated_std': aggregated_std,
            'aggregated_cv': aggregated_cv
        }

        return features.numpy(), result

    except Exception as e:
        print(f"\n‚ùå Feature extraction failed!")
        print(f"  Error: {e}")
        import traceback
        traceback.print_exc()
        return None, None


def validate_features(features: np.ndarray) -> Dict:
    """Run empirical validations on extracted features."""
    print("\n" + "="*70)
    print("EMPIRICAL VALIDATION")
    print("="*70)

    results = {}

    # Validation 1: No invalid values (NaN/Inf check, not magnitude check)
    print("\n1. Numerical Validity Check")
    max_abs = np.abs(features).max()
    has_inf = np.isinf(features).any()
    has_invalid_nan = np.isnan(features).all(axis=0).any()  # All-NaN columns are invalid

    print(f"   Max absolute value: {max_abs:.2e}")
    print(f"   Contains Inf: {has_inf}")
    print(f"   Contains all-NaN features: {has_invalid_nan}")

    if has_inf or has_invalid_nan:
        print(f"   ‚ùå Found invalid numerical values (Inf or all-NaN columns)")
        if has_inf:
            inf_mask = np.isinf(features)
            num_inf = inf_mask.sum()
            print(f"      Inf count: {num_inf}")
        if has_invalid_nan:
            all_nan_features = np.where(np.isnan(features).all(axis=0))[0]
            print(f"      All-NaN feature indices: {all_nan_features.tolist()}")
        results['extreme_values'] = False
    else:
        print(f"   ‚úÖ No invalid values (large magnitudes accepted as valid)")
        print(f"      Note: Large values (e.g., ~1e8) are valid for some features")
        results['extreme_values'] = True

    # Validation 2: NaN count
    print("\n2. NaN Count Check")
    nan_count = np.isnan(features).sum()
    nan_pct = 100.0 * nan_count / features.size

    print(f"   NaN count: {nan_count} / {features.size} ({nan_pct:.3f}%)")

    if nan_pct < 5.0:  # Allow up to 5% NaN (for mathematically undefined cases)
        print(f"   ‚úÖ NaN percentage acceptable (Phase 1 fixes working)")
        results['nan_count'] = True
    else:
        print(f"   ‚ùå Too many NaN values ({nan_pct:.1f}%)")
        results['nan_count'] = False

    # Validation 3: Zero-variance features
    print("\n3. Zero-Variance Check")
    variance = np.nanvar(features, axis=0)
    num_zero_var = (variance < 1e-12).sum()

    print(f"   Zero-variance features: {num_zero_var} / {features.shape[1]}")

    if num_zero_var == 0:
        print(f"   ‚úÖ No zero-variance features (Phase 2 extractors working)")
        results['zero_variance'] = True
    else:
        print(f"   ‚ö†Ô∏è  Found {num_zero_var} zero-variance features")
        # Identify which features
        zero_var_indices = np.where(variance < 1e-12)[0]
        print(f"      Zero-variance feature indices: {zero_var_indices.tolist()}")
        # Print values of first few
        for idx in zero_var_indices[:5]:
            unique_vals = np.unique(features[:, idx])
            print(f"         Feature {idx}: unique values = {unique_vals}")
        results['zero_variance'] = False

    # Validation 4: Feature count
    print("\n4. Feature Count Check")
    num_features = features.shape[1]

    print(f"   Total aggregated features: {num_features}")
    print(f"   Expected: ~288 (159 base √ó 3 aggregations - some per-trajectory)")

    # With all Phase 2 extractors enabled: ~159 base features
    # Aggregated: per-timestep get 3√ó (mean/std/cv), per-trajectory stay 1√ó
    # Expected: ~288 aggregated features
    if 280 <= num_features <= 300:
        print(f"   ‚úÖ Feature count meets expectations")
        results['feature_count'] = True
    else:
        print(f"   ‚ùå Feature count outside expected range ({num_features} not in [280, 300])")
        results['feature_count'] = False

    # Validation 5: Feature statistics
    print("\n5. Feature Statistics")
    mean = np.nanmean(features, axis=0)
    std = np.nanstd(features, axis=0)

    print(f"   Mean of means: {mean.mean():.3f} ¬± {mean.std():.3f}")
    print(f"   Mean of stds: {std.mean():.3f} ¬± {std.std():.3f}")
    print(f"   Variance range: [{variance.min():.2e}, {variance.max():.2e}]")

    # Check for suspicious patterns
    high_std_count = (std > 100).sum()
    if high_std_count > 0:
        print(f"   ‚ö†Ô∏è  {high_std_count} features with std > 100 (may indicate remaining outliers)")

    results['statistics'] = True

    return results


def main():
    # Test feature extraction
    features, result = test_feature_extraction()

    if features is None:
        print("\n‚ùå Feature extraction failed. Cannot proceed with validation.")
        return 1

    # Validate features
    results = validate_features(features)

    # Summary
    print("\n" + "="*70)
    print("VALIDATION SUMMARY")
    print("="*70)

    validations = [
        ('Numerical Validity (no Inf/all-NaN)', results.get('extreme_values', False)),
        ('NaN Count (Phase 1 fixes)', results.get('nan_count', False)),
        ('No Zero-Variance (Phase 2)', results.get('zero_variance', False)),
        ('Feature Count (correct)', results.get('feature_count', False)),
        ('Statistics', results.get('statistics', False)),
    ]

    passed_count = sum(1 for _, passed in validations if passed)
    total_count = len(validations)

    for name, passed in validations:
        status = "‚úÖ PASS" if passed else "‚ùå FAIL"
        print(f"  {status}: {name}")

    print("\n" + "="*70)
    print(f"Overall: {passed_count}/{total_count} validations passed")
    print("="*70)

    if passed_count == total_count:
        print("\nüéâ All validations passed! SDF v2.1 features are working correctly.")
        print("\nPhase 1 (Numerical Stability): ‚úÖ")
        print("  - FFT power normalization working")
        print("  - Kurtosis/skewness adaptive clipping working")
        print("  - T-normalization working")
        print("  - NaN handling working")
        print("\nPhase 2 (New Extractors): ‚úÖ")
        print("  - Distributional features implemented")
        print("  - Structural features implemented")
        print("  - Physics features implemented")
        print("  - Morphological features implemented")
        print("  - Multiscale features implemented")
        return 0
    else:
        print(f"\n‚ö†Ô∏è  {total_count - passed_count} validation(s) failed. Review above for details.")
        return 1


if __name__ == "__main__":
    sys.exit(main())
